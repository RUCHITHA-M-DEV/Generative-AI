{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Unit 2 - Part 1a: LangChain Setup & Models**"
      ],
      "metadata": {
        "id": "tdfVJwbHWAL4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzElVQA5saac",
        "outputId": "3c8954f7-ce2d-4ca7-98c7-88c48ba11914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/500.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m491.5/500.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.5/500.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "5kOj0hkbu6iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFItd6l6uUSw",
        "outputId": "e32b2252-c622-4000-da60-31c215ea4054"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "#Model A: The \"Accountant\" (Precision)\n",
        "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "\n",
        "#Modek B: The\n",
        "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
      ],
      "metadata": {
        "id": "RWXEChGdvcLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'Idea' in one sentence.\"\n",
        "\n",
        "print(\"--- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTrA0rJiwcg2",
        "outputId": "9776e707-9cb9-47a4-a7bc-9c98f59287ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a thought, concept, or mental image formed in the mind.\n",
            "Run 2: An idea is a thought, concept, or suggestion that is formed or exists in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- CREATIVE (Temp=1) ---\")\n",
        "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaclSE7jS7ga",
        "outputId": "184d5364-6e63-4372-a1ee-e18074362e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CREATIVE (Temp=1) ---\n",
            "Run 1: An idea is a thought, suggestion, or concept that arises in the mind, often as a potential solution, plan, or understanding.\n",
            "Run 2: An idea is a thought, concept, or plan that exists in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unit 2 - Part 1b: Prompts & Parsers**"
      ],
      "metadata": {
        "id": "2gVfblr7WNl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup from Part 1a (Hidden for brevity)\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key:\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "VTAKEuTex3dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#System message = controls tone + behavior\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "#Scenario: Make the AI rude.\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a friendly geography tutor who explains answers clearly and simply.\"),\n",
        "    HumanMessage(content=\"What is the capital of France?\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn4nr0Q4y2iV",
        "outputId": "841779c4-32a6-4c41-f7c7-981a4faacb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's a great question!\n",
            "\n",
            "The capital of France is **Paris**.\n",
            "\n",
            "It's a beautiful city, famous for landmarks like the Eiffel Tower, the Louvre Museum, and Notre Dame Cathedral. Have you ever heard of it before?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a translator. Translate {input_language} to {output_language}.\"),\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "# We can check what inputs it expects\n",
        "print(f\"Required variables: {template.input_variables}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feHEgT7STrzW",
        "outputId": "cf4883e9-0d15-4094-f5a0-10c1a43840b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required variables: ['input_language', 'output_language', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Raw Message\n",
        "raw_msg = llm.invoke(\"Hi\")\n",
        "print(f\"Raw Type: {type(raw_msg)}\")\n",
        "\n",
        "# Parsed String\n",
        "clean_text = parser.invoke(raw_msg)\n",
        "print(f\"Parsed Type: {type(clean_text)}\")\n",
        "print(f\"Content: {clean_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUkuKNX1TzL4",
        "outputId": "f455a1c9-27a7-4b1a-a08d-c90a32718353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
            "Parsed Type: <class 'str'>\n",
            "Content: Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"]=getpass.getpass(\"Enter your Google API Key:\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}\")\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "7FP5qfcx08UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unit 2 - Part 1c: LCEL (LangChain Expression Language)**"
      ],
      "metadata": {
        "id": "zYo1vVxBT9Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_value = template.invoke({\"topic\": \"Ice cream\"})\n",
        "\n",
        "response_obj = llm.invoke(prompt_value)\n",
        "\n",
        "final_text = parser.invoke(response_obj)\n",
        "\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_xpkLJP1hSp",
        "outputId": "cdc2a333-e00b-4ca4-db94-e496a87de4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about ice cream:\n",
            "\n",
            "The **ice cream cone** was popularized, if not outright invented, by accident at the **1904 St. Louis World's Fair**!\n",
            "\n",
            "An ice cream vendor ran out of dishes, and a Syrian waffle vendor next door, named Ernest Hamwi, rolled his crispy waffles into cones to help out. This ingenious solution became an instant hit and revolutionized how people enjoyed their ice cream!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = template | llm | parser\n",
        "\n",
        "print(chain.invoke({\"topic\": \"Octopuses\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPdN-fwa2SGb",
        "outputId": "0381e5c0-3792-43f8-a6d9-1508aec338f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun fact about octopuses:\n",
            "\n",
            "Octopuses have **three hearts** and **blue blood!**\n",
            "\n",
            "Two hearts pump blood through their gills, and a third, larger heart circulates blood to the rest of their body. Their blood is blue because it uses a copper-based protein called hemocyanin to transport oxygen, instead of the iron-based hemoglobin that makes our blood red.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**\n",
        "\n",
        "Create a chain that:\n",
        "\n",
        "Takes a movie name.\n",
        "Asks for its release year.\n",
        "Calculates how many years ago that was (You can try just asking the LLM to do the math).\n",
        "Try to do it in one line of LCEL."
      ],
      "metadata": {
        "id": "NEqXYu47REFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_template(\n",
        "    \"Tell me the release year of the movie '{movie}'. \"\n",
        "    \"Assume the current year is {current_year}. \"\n",
        "    \"Return EXACTLY in this format:\\n\"\n",
        "    \"Movie Name: {movie}\\n\"\n",
        "    \"{movie} was released in YEAR. It was released X years ago.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "tF8nJIymRMJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = template | llm | parser\n",
        "\n",
        "print(chain.invoke({\n",
        "    \"movie\": \"Inception\",\n",
        "    \"current_year\": 2026\n",
        "}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEUmU6zhRXd4",
        "outputId": "1262b398-3b8d-4209-de37-6979a22cd77d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movie Name: Inception\n",
            "Inception was released in 2010. It was released 16 years ago.\n"
          ]
        }
      ]
    }
  ]
}